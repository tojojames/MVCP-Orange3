% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}

\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
  \DeclareUnicodeCharacter{00A0}{\nobreakspace}
\else\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}
\usepackage{times}
\usepackage[Sonny]{fncychap}
\usepackage{longtable}
\usepackage{sphinx}
\usepackage{multirow}
\usepackage{eqparbox}


\addto\captionsenglish{\renewcommand{\figurename}{Fig.\@ }}
\addto\captionsenglish{\renewcommand{\tablename}{Table }}
\SetupFloatingEnvironment{literal-block}{name=Listing }

\addto\extrasenglish{\def\pageautorefname{page}}

\setcounter{tocdepth}{3}


\title{Orange - Conformal Prediction Documentation}
\date{Jul 19, 2016}
\release{1.0}
\author{Biolab}
\newcommand{\sphinxlogo}{}
\renewcommand{\releasename}{Release}
\makeindex

\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\expandafter\def\csname PYG@tok@w\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PYG@tok@sd\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@gs\endcsname{\let\PYG@bf=\textbf}
\expandafter\def\csname PYG@tok@o\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PYG@tok@il\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gd\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@s\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nl\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.13,0.44}{##1}}}
\expandafter\def\csname PYG@tok@go\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.20,0.20,0.20}{##1}}}
\expandafter\def\csname PYG@tok@gp\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@ss\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.32,0.47,0.09}{##1}}}
\expandafter\def\csname PYG@tok@sx\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.78,0.36,0.04}{##1}}}
\expandafter\def\csname PYG@tok@sh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@se\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@nt\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.45}{##1}}}
\expandafter\def\csname PYG@tok@m\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@ni\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.84,0.33,0.22}{##1}}}
\expandafter\def\csname PYG@tok@mh\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@c1\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@nf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.02,0.16,0.49}{##1}}}
\expandafter\def\csname PYG@tok@gh\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@kp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@no\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.38,0.68,0.84}{##1}}}
\expandafter\def\csname PYG@tok@mb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@s1\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@na\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@kt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.56,0.13,0.00}{##1}}}
\expandafter\def\csname PYG@tok@kr\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@gi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PYG@tok@vc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@nb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@cs\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\colorbox[rgb]{1.00,0.94,0.94}{\strut ##1}}}
\expandafter\def\csname PYG@tok@ow\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@cp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@nc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.05,0.52,0.71}{##1}}}
\expandafter\def\csname PYG@tok@k\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@bp\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@kn\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@mo\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@kd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@sc\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@si\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.44,0.63,0.82}{##1}}}
\expandafter\def\csname PYG@tok@sb\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}
\expandafter\def\csname PYG@tok@sr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.14,0.33,0.53}{##1}}}
\expandafter\def\csname PYG@tok@vi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@ne\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@ch\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@gr\endcsname{\def\PYG@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PYG@tok@nv\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@ge\endcsname{\let\PYG@it=\textit}
\expandafter\def\csname PYG@tok@mi\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@cpf\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@gu\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PYG@tok@nd\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.33,0.33,0.33}{##1}}}
\expandafter\def\csname PYG@tok@c\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@mf\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.13,0.50,0.31}{##1}}}
\expandafter\def\csname PYG@tok@gt\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PYG@tok@vg\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.38,0.84}{##1}}}
\expandafter\def\csname PYG@tok@cm\endcsname{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.25,0.50,0.56}{##1}}}
\expandafter\def\csname PYG@tok@kc\endcsname{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.44,0.13}{##1}}}
\expandafter\def\csname PYG@tok@err\endcsname{\def\PYG@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PYG@tok@s2\endcsname{\def\PYG@tc##1{\textcolor[rgb]{0.25,0.44,0.63}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\renewcommand\PYGZsq{\textquotesingle}

\begin{document}

\maketitle
\tableofcontents
\phantomsection\label{index::doc}



\chapter{Tutorial}
\label{tutorial:welcome-to-orange-conformal-prediction-s-documentation}\label{tutorial:tutorial}\label{tutorial::doc}

\section{Introduction}
\label{tutorial:introduction}
The Conformal Predictions add-on expands the Orange library with
implementations of algorithms from the theoretical framework of conformal
predictions (CP) to obtain error calibration under classification and
regression settings.

In contrast with standard supervised machine learning, which for a given new
data instance typically produces \(\hat{y}\), called a \emph{point prediction}, here we are
interested in making a \emph{region prediction}. For example, with conformal
prediction we could produce a 95\% prediction region --- a set \(\Gamma^{0.05}\)
that contains the true label \(y\) with probability at least 95\%.  In the case of
regression, where \(y\) is a number, \(\Gamma^{0.05}\) is typically an interval around \(\hat{y}\).
In the case of classification, where \(y\) has a limited number of possible values,
\(\Gamma^{0.05}\) may consist of a few of these values or, in the ideal case, just one.
For a more detailed explanation of the conformal predictions theory refer to the paper \phantomsection\label{tutorial:id1}{\hyperref[tutorial:vovk08]{\crossref{{[}Vovk08{]}}}}
or the book \phantomsection\label{tutorial:id2}{\hyperref[tutorial:shafer05]{\crossref{{[}Shafer05{]}}}}.

In this library the final method for conformal predictions is obtained by
selecting a combination of pre-prepared components.  Starting with the learning
method (either classification or regression) used to fit predictive models, we
need to link it with a suitable nonconformity measure and use them together in
a selected conformal predictions procedure: transductive, inductive or cross.
These CP procedures differ in the way data is split and used for training the
predictive model and calibration, which computes the distribution of
nonconformity scores used to evaluate possible new predicitions. Inductive CP
requires two disjoint data sets to be provided - one for training, the other
for calibration. Cross CP uses a single training data set and automatically
prepares k different splits into training and calibration sets in the same
manner as k-fold crossvalidation. Transductive CP on the other hand does not
need a separate calibration set at all, but retrains the model with a new test
instance included for each of its possible labels and compares the
nonconformity to those of the labelled instances. This allows it to use the
complete training set, but makes it computationally more expensive.

Sections below will explain how to use the implemented methods from this
library through practical examples and use-cases. For a detailed documentation
of implemented methods and classes along with their parameters consult the
{\hyperref[cp:library\string-reference]{\crossref{\DUrole{std,std-ref}{Library reference}}}}.  For more code examples, take a look at the tests
module.


\subsection{References}
\label{tutorial:references}

\section{Classification}
\label{tutorial:classification}
All 3 types of conformal prediction are implemented for classification
(transductive, inductive and cross), with several different nonconformity
measures to choose from.

We will show how to train and use a conformal predictive model in the following
simple, but fully functional example.

Let's load the iris data set and try to make a prediction for the last
instance using the rest for learning.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{Orange}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{iris} \PYG{o}{=} \PYG{n}{Orange}\PYG{o}{.}\PYG{n}{data}\PYG{o}{.}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train} \PYG{o}{=} \PYG{n}{iris}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{test\PYGZus{}instance} \PYG{o}{=} \PYG{n}{iris}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{Verbatim}

We will use a LogisticRegressionLearner from Orange and the inverse probability
nonconformity score in a 5-fold cross conformal prediction classifier.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{lr} \PYG{o}{=} \PYG{n}{Orange}\PYG{o}{.}\PYG{n}{classification}\PYG{o}{.}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ip} \PYG{o}{=} \PYG{n}{cp}\PYG{o}{.}\PYG{n}{nonconformity}\PYG{o}{.}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{lr}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ccp} \PYG{o}{=} \PYG{n}{cp}\PYG{o}{.}\PYG{n}{classification}\PYG{o}{.}\PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{ip}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\end{Verbatim}

Predicting the 90\% and 99\% prediction regions gives the following results.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Actual class:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{test\PYGZus{}instance}\PYG{o}{.}\PYG{n}{get\PYGZus{}class}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{Actual class: Iris\PYGZhy{}virginica}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ccp}\PYG{p}{(}\PYG{n}{test\PYGZus{}instance}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{[\PYGZsq{}Iris\PYGZhy{}virginica\PYGZsq{}]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ccp}\PYG{p}{(}\PYG{n}{test\PYGZus{}instance}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{[\PYGZsq{}Iris\PYGZhy{}versicolor\PYGZsq{}, \PYGZsq{}Iris\PYGZhy{}virginica\PYGZsq{}]}
\end{Verbatim}

We can see that in the first case only the correct class of `Iris-virginica'
was predicted.  In the second case, with a much lower tolerance for errors, the
model claims only that the instance belongs to one of two possible classes
`Iris-versicolor' or `Iris-virginica', but not the third `Iris-setosa'.


\section{Regression}
\label{tutorial:regression}
For regression inductive and cross conformal prediction are implemented along
with several nonconformity measures.

Similarly to the classification example, let's combine some standard components
to show how to train and use a conformal prediction model for regression.

Let's load the housing data set and try to make a prediction for the last
instance using the rest for learning.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{Orange}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{housing} \PYG{o}{=} \PYG{n}{Orange}\PYG{o}{.}\PYG{n}{data}\PYG{o}{.}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train} \PYG{o}{=} \PYG{n}{housing}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{test\PYGZus{}instance} \PYG{o}{=} \PYG{n}{housing}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}
\end{Verbatim}

We will use a LinearRegressionLearner from Orange and the absolute error
nonconformity score in a 5-fold cross conformal regressor.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{lr} \PYG{o}{=} \PYG{n}{Orange}\PYG{o}{.}\PYG{n}{regression}\PYG{o}{.}\PYG{n}{LinearRegressionLearner}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{abs\PYGZus{}err} \PYG{o}{=} \PYG{n}{cp}\PYG{o}{.}\PYG{n}{nonconformity}\PYG{o}{.}\PYG{n}{AbsError}\PYG{p}{(}\PYG{n}{lr}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ccr} \PYG{o}{=} \PYG{n}{cp}\PYG{o}{.}\PYG{n}{regression}\PYG{o}{.}\PYG{n}{CrossRegressor}\PYG{p}{(}\PYG{n}{abs\PYGZus{}err}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\end{Verbatim}

Predicting the 90\% and 99\% prediction regions gives the following results.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Actual target value:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{test\PYGZus{}instance}\PYG{o}{.}\PYG{n}{get\PYGZus{}class}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{Actual target value: 11.900}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ccr}\PYG{p}{(}\PYG{n}{test\PYGZus{}instance}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{(13.708550425853684, 31.417230194137165)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ccr}\PYG{p}{(}\PYG{n}{test\PYGZus{}instance}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{(\PYGZhy{}0.98542733224618217, 46.111207952237031)}
\end{Verbatim}

We can see that in the first case the predicted interval was smaller, but did
not contain the correct value (this should not happend more than 10\% of the
time). In the second case, with a much lower tolerance for errors, the model
predicted a larger interval, which did contain the correct value.


\section{Evaluation}
\label{tutorial:evaluation}
The evaluation module provides many useful classes and functions for evaluating the performance and validity of conformal predictions.
The main two classes, which represent the results of a conformal classifier and regressor, are {\hyperref[cp.evaluation:cp.evaluation.ResultsClass]{\crossref{\code{cp.evaluation.ResultsClass}}}} and {\hyperref[cp.evaluation:cp.evaluation.ResultsRegr]{\crossref{\code{cp.evaluation.ResultsRegr}}}}.

For ease of use, the evaluation results can be obtained using utility functions that evaluate the selected conformal predictor on data defined by the provided sampler ({\hyperref[cp.evaluation:cp.evaluation.run]{\crossref{\code{cp.evaluation.run()}}}}) or explicitly provided by the user ({\hyperref[cp.evaluation:cp.evaluation.run_train_test]{\crossref{\code{cp.evaluation.run\_train\_test()}}}}).

As an example, let's take a look at how to quickly evaluate a conformal classifier on a test data set and compute some of the performance metrics:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{Orange}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{import} \PYG{n+nn}{cp}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{iris} \PYG{o}{=} \PYG{n}{Orange}\PYG{o}{.}\PYG{n}{data}\PYG{o}{.}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n}{iris}\PYG{p}{[}\PYG{p}{:}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{iris}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{:}\PYG{l+m+mi}{2}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{lr} \PYG{o}{=} \PYG{n}{Orange}\PYG{o}{.}\PYG{n}{classification}\PYG{o}{.}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ip} \PYG{o}{=} \PYG{n}{cp}\PYG{o}{.}\PYG{n}{nonconformity}\PYG{o}{.}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{lr}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ccp} \PYG{o}{=} \PYG{n}{cp}\PYG{o}{.}\PYG{n}{classification}\PYG{o}{.}\PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{ip}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{res} \PYG{o}{=} \PYG{n}{cp}\PYG{o}{.}\PYG{n}{evaluation}\PYG{o}{.}\PYG{n}{run\PYGZus{}train\PYGZus{}test}\PYG{p}{(}\PYG{n}{ccp}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{train}\PYG{p}{,} \PYG{n}{test}\PYG{p}{)}
\end{Verbatim}

The results are an instance of {\hyperref[cp.evaluation:cp.evaluation.ResultsClass]{\crossref{\code{cp.evaluation.ResultsClass}}}} mentioned above, and can be used to compute the accuracy of predictions (fraction of predictions including the actual class). For a \emph{valid} predictor it needs to hold that the error (1 - accuracy) is lower or equal to the specified significance level.
In addition to \emph{validity}, we are often interested in the \emph{efficiency} of a predictor. For classification, this is often measured with the fraction of cases with a single predicted class ({\hyperref[cp.evaluation:cp.evaluation.ResultsClass.singleton_criterion]{\crossref{\code{cp.evaluation.ResultsClass.singleton\_criterion()}}}}). For regression, one might measure the widths of predicted intervals and e.g. report the average value ({\hyperref[cp.evaluation:cp.evaluation.ResultsRegr.mean_range]{\crossref{\code{cp.evaluation.ResultsRegr.mean\_range()}}}}).

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Accuracy:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{res}\PYG{o}{.}\PYG{n}{accuracy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{Accuracy: 0.946666666667}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Singletons:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{res}\PYG{o}{.}\PYG{n}{singleton\PYGZus{}criterion}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{Singletons: 0.96}
\end{Verbatim}

Another very useful visual validation approach is to plot the dependency of the actual measured error rate at different levels of the specified significance so the user can quickly see that the error is indeed controlled by the parameter.
There is a function in the evaluation module that prepares a calibration plot for the specified predictor and data:

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp}\PYG{o}{.}\PYG{n}{evaluation}\PYG{o}{.}\PYG{n}{calibration\PYGZus{}plot}\PYG{p}{(}\PYG{n}{ccp}\PYG{p}{,} \PYG{n}{iris}\PYG{p}{,} \PYG{n}{fname}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{calibration.png}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{Verbatim}


\chapter{Library reference}
\label{cp:library-reference}\label{cp:id1}\label{cp::doc}

\section{cp.base}
\label{cp.base:module-cp.base}\label{cp.base::doc}\label{cp.base:cp-base}\index{cp.base (module)}\index{ConformalPredictor (class in cp.base)}

\begin{fulllineitems}
\phantomsection\label{cp.base:cp.base.ConformalPredictor}\pysigline{\strong{class }\code{cp.base.}\bfcode{ConformalPredictor}}
Bases: \code{object}

Base class for conformal predictors.
\index{\_\_call\_\_() (cp.base.ConformalPredictor method)}

\begin{fulllineitems}
\phantomsection\label{cp.base:cp.base.ConformalPredictor.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{example}, \emph{eps}}{}
Extending classes should implement this method to return predicted values
for a given example and significance level.

\end{fulllineitems}

\index{predict() (cp.base.ConformalPredictor method)}

\begin{fulllineitems}
\phantomsection\label{cp.base:cp.base.ConformalPredictor.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{example}, \emph{eps}}{}
Extending classes should implement this method to return a prediction object.
for a given example and significance level.

\end{fulllineitems}


\end{fulllineitems}



\section{cp.classification}
\label{cp.classification:cp-classification}\label{cp.classification:module-cp.classification}\label{cp.classification::doc}\index{cp.classification (module)}
Classification module contains methods for conformal classification.

Conformal classifiers predict a set of classes (not always a single class) under a given
significance level (error rate). Every classifier works in combination with a nonconformity measure
and on average predicts the correct class with the given error rate. Lower error rates result in
smaller sets of predicted classes.

Structure:
\begin{itemize}
\item {} \begin{description}
\item[{ConformalClassifier}] \leavevmode\begin{itemize}
\item {} 
Transductive ({\hyperref[cp.classification:cp.classification.TransductiveClassifier]{\crossref{\code{TransductiveClassifier}}}})

\item {} 
Inductive ({\hyperref[cp.classification:cp.classification.InductiveClassifier]{\crossref{\code{InductiveClassifier}}}})

\item {} 
Cross ({\hyperref[cp.classification:cp.classification.CrossClassifier]{\crossref{\code{CrossClassifier}}}})

\end{itemize}

\end{description}

\end{itemize}
\index{PredictionClass (class in cp.classification)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.PredictionClass}\pysiglinewithargsret{\strong{class }\code{cp.classification.}\bfcode{PredictionClass}}{\emph{p}, \emph{eps}}{}
Bases: \code{object}

Conformal classification prediction object,
which is produced by the {\hyperref[cp.classification:cp.classification.ConformalClassifier.predict]{\crossref{\code{ConformalClassifier.predict()}}}} method.
\index{p (cp.classification.PredictionClass attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.PredictionClass.p}\pysigline{\bfcode{p}}
\emph{List} -- List of pairs (p-value, class)

\end{fulllineitems}

\index{eps (cp.classification.PredictionClass attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.PredictionClass.eps}\pysigline{\bfcode{eps}}
\emph{float} -- Default significance level (error rate).

\end{fulllineitems}

\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{tcp} \PYG{o}{=} \PYG{n}{TransductiveClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{NaiveBayesLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{prediction} \PYG{o}{=} \PYG{n}{tcp}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{prediction}\PYG{o}{.}\PYG{n}{confidence}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{prediction}\PYG{o}{.}\PYG{n}{credibility}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{prediction} \PYG{o}{=} \PYG{n}{tcp}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{prediction}\PYG{o}{.}\PYG{n}{classes}\PYG{p}{(}\PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{prediction}\PYG{o}{.}\PYG{n}{classes}\PYG{p}{(}\PYG{l+m+mf}{0.9}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.classification.PredictionClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.PredictionClass.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{p}, \emph{eps}}{}
Initialize the prediction.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{p}} (\emph{\texttt{List}}) -- List of pairs (p-value, class)

\item {} 
\textbf{\texttt{eps}} (\emph{\texttt{float}}) -- Default significance level (error rate).

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{classes() (cp.classification.PredictionClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.PredictionClass.classes}\pysiglinewithargsret{\bfcode{classes}}{\emph{eps=None}}{}
Compute the set of classes under the default or given \titleref{eps} value.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{eps}} (\emph{\texttt{float}}) -- Significance level (error rate).

\item[{Returns}] \leavevmode
List of predicted classes.

\end{description}\end{quote}

\end{fulllineitems}

\index{verdict() (cp.classification.PredictionClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.PredictionClass.verdict}\pysiglinewithargsret{\bfcode{verdict}}{\emph{ref}}{}
Conformal classification prediction is correct when the actual class appears
among the predicted classes.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{ref}} -- Reference/actual class

\item[{Returns}] \leavevmode
True if the prediction with default \titleref{eps} is correct.

\end{description}\end{quote}

\end{fulllineitems}

\index{confidence() (cp.classification.PredictionClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.PredictionClass.confidence}\pysiglinewithargsret{\bfcode{confidence}}{}{}
Confidence is an efficiency measure of a single prediction.

Computes minimum \(\mathit{eps}\) that would still result in a prediction of a single label.
\(\mathit{eps} = \text{second\_largest}(p_i)\)
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Confidence \(1-\mathit{eps}\).

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{credibility() (cp.classification.PredictionClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.PredictionClass.credibility}\pysiglinewithargsret{\bfcode{credibility}}{}{}
Credibility is an efficiency measure of a single prediction.
Small credibility indicates an unusual example.

Computes minimum \(\mathit{eps}\) that would result in an empty prediction set.
\(\mathit{eps} = \text{max}(p_i)\)
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
Credibility \(\mathit{eps}\).

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ConformalClassifier (class in cp.classification)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.ConformalClassifier}\pysiglinewithargsret{\strong{class }\code{cp.classification.}\bfcode{ConformalClassifier}}{\emph{nc\_measure}, \emph{mondrian=False}}{}
Bases: {\hyperref[cp.base:cp.base.ConformalPredictor]{\crossref{\code{cp.base.ConformalPredictor}}}}

Base class for conformal classifiers.
\index{\_\_init\_\_() (cp.classification.ConformalClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.ConformalClassifier.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}, \emph{mondrian=False}}{}
Verify that the nonconformity measure can be used for classification.

\end{fulllineitems}

\index{p\_values() (cp.classification.ConformalClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.ConformalClassifier.p_values}\pysiglinewithargsret{\bfcode{p\_values}}{\emph{example}}{}
Extending classes should implement this method to return a list of pairs (p-value, class)
for a given example.

Conformal classifier assigns an assumed class value to the given example and computes its nonconformity.
P-value is the ratio of more nonconformal (stranger) instances that the given example.

\end{fulllineitems}

\index{predict() (cp.classification.ConformalClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.ConformalClassifier.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{example}, \emph{eps=None}}{}
Compute a classification prediction object from p-values for a given example and significance level.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{example}} (\emph{\texttt{ndarray}}) -- Attributes array.

\item {} 
\textbf{\texttt{eps}} (\emph{\texttt{float}}) -- Default significance level (error rate).

\end{itemize}

\item[{Returns}] \leavevmode
Classification prediction object.

\item[{Return type}] \leavevmode
{\hyperref[cp.classification:cp.classification.PredictionClass]{\crossref{PredictionClass}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_\_call\_\_() (cp.classification.ConformalClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.ConformalClassifier.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{example}, \emph{eps}}{}
Compute predicted classes for a given example and significance level.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{example}} (\emph{\texttt{ndarray}}) -- Attributes array.

\item {} 
\textbf{\texttt{eps}} (\emph{\texttt{float}}) -- Significance level (error rate).

\end{itemize}

\item[{Returns}] \leavevmode
List of predicted classes.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{TransductiveClassifier (class in cp.classification)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.TransductiveClassifier}\pysiglinewithargsret{\strong{class }\code{cp.classification.}\bfcode{TransductiveClassifier}}{\emph{nc\_measure}, \emph{train=None}, \emph{mondrian=False}}{}
Bases: {\hyperref[cp.classification:cp.classification.ConformalClassifier]{\crossref{\code{cp.classification.ConformalClassifier}}}}

Transductive classification.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{tcp} \PYG{o}{=} \PYG{n}{TransductiveClassifier}\PYG{p}{(}\PYG{n}{ProbabilityMargin}\PYG{p}{(}\PYG{n}{NaiveBayesLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tcp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.classification.TransductiveClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.TransductiveClassifier.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}, \emph{train=None}, \emph{mondrian=False}}{}
Initialize transductive classifier with a nonconformity measure and a training set.

Fit the conformal classifier to the training set if present.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{nc\_measure}} ({\hyperref[cp.nonconformity:cp.nonconformity.ClassNC]{\crossref{\emph{\texttt{ClassNC}}}}}) -- Classification nonconformity measure.

\item {} 
\textbf{\texttt{train}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a training set.

\item {} 
\textbf{\texttt{mondrian}} (\emph{\texttt{bool}}) -- Use a mondrian setting for computing p-values.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{fit() (cp.classification.TransductiveClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.TransductiveClassifier.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{train}}{}
Fit the conformal classifier to the training set and store the domain.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{train}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a training set.

\end{description}\end{quote}

\end{fulllineitems}

\index{p\_values() (cp.classification.TransductiveClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.TransductiveClassifier.p_values}\pysiglinewithargsret{\bfcode{p\_values}}{\emph{example}}{}
Compute p-values for every possible class.

Transductive classifier appends the given example with an assumed class value to the training set
and compares its nonconformity against all other instances.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{example}} (\emph{\texttt{ndarray}}) -- Attributes array.

\item[{Returns}] \leavevmode
List of pairs (p-value, class)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{InductiveClassifier (class in cp.classification)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.InductiveClassifier}\pysiglinewithargsret{\strong{class }\code{cp.classification.}\bfcode{InductiveClassifier}}{\emph{nc\_measure}, \emph{train=None}, \emph{calibrate=None}, \emph{mondrian=False}}{}
Bases: {\hyperref[cp.classification:cp.classification.ConformalClassifier]{\crossref{\code{cp.classification.ConformalClassifier}}}}

Inductive classification.
\index{alpha (cp.classification.InductiveClassifier attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.InductiveClassifier.alpha}\pysigline{\bfcode{alpha}}
Nonconformity scores of the calibration instances. Computed by the {\hyperref[cp.classification:cp.classification.InductiveClassifier.fit]{\crossref{\code{fit()}}}} method.

\end{fulllineitems}

\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{calibrate} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{RandomSampler}\PYG{p}{(}\PYG{n}{train}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{icp} \PYG{o}{=} \PYG{n}{InductiveClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{,} \PYG{n}{calibrate}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{icp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.classification.InductiveClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.InductiveClassifier.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}, \emph{train=None}, \emph{calibrate=None}, \emph{mondrian=False}}{}
Initialize inductive classifier with a nonconformity measure, training set and calibration set.
If present, fit the conformal classifier to the training set and compute the nonconformity scores of
calibration set.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{nc\_measure}} ({\hyperref[cp.nonconformity:cp.nonconformity.ClassNC]{\crossref{\emph{\texttt{ClassNC}}}}}) -- Classification nonconformity measure.

\item {} 
\textbf{\texttt{train}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a training set.

\item {} 
\textbf{\texttt{calibrate}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a calibration set.

\item {} 
\textbf{\texttt{mondrian}} (\emph{\texttt{bool}}) -- Use a mondrian setting for computing p-values.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{fit() (cp.classification.InductiveClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.InductiveClassifier.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{train}, \emph{calibrate}}{}
Fit the conformal classifier to the training set, compute and store nonconformity scores ({\hyperref[cp.classification:cp.classification.InductiveClassifier.alpha]{\crossref{\code{alpha}}}})
on the calibration set and store the domain.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{train}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a training set.

\item {} 
\textbf{\texttt{calibrate}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a calibration set.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{p\_values() (cp.classification.InductiveClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.InductiveClassifier.p_values}\pysiglinewithargsret{\bfcode{p\_values}}{\emph{example}}{}
Compute p-values for every possible class.

Inductive classifier assigns an assumed class value to the given example and compares its nonconformity
against all other instances in the calibration set.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{example}} (\emph{\texttt{ndarray}}) -- Attributes array.

\item[{Returns}] \leavevmode
List of pairs (p-value, class)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{CrossClassifier (class in cp.classification)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.CrossClassifier}\pysiglinewithargsret{\strong{class }\code{cp.classification.}\bfcode{CrossClassifier}}{\emph{nc\_measure}, \emph{k}, \emph{train=None}, \emph{mondrian=False}}{}
Bases: {\hyperref[cp.classification:cp.classification.InductiveClassifier]{\crossref{\code{cp.classification.InductiveClassifier}}}}

Cross classification.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ccp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ccp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.classification.CrossClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.CrossClassifier.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}, \emph{k}, \emph{train=None}, \emph{mondrian=False}}{}
Initialize cross classifier with a nonconformity measure, number of folds and training set.
If present, fit the conformal classifier to the training set.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{nc\_measure}} ({\hyperref[cp.nonconformity:cp.nonconformity.ClassNC]{\crossref{\emph{\texttt{ClassNC}}}}}) -- Classification nonconformity measure.

\item {} 
\textbf{\texttt{k}} (\emph{\texttt{int}}) -- Number of folds.

\item {} 
\textbf{\texttt{train}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a training set.

\item {} 
\textbf{\texttt{mondrian}} (\emph{\texttt{bool}}) -- Use a mondrian setting for computing p-values.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{fit() (cp.classification.CrossClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.CrossClassifier.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{train}}{}
Fit the cross classifier to the training set. Split the training set into k folds for use as
training and calibration set with an inductive classifier. Concatenate the computed nonconformity scores
and store them ({\hyperref[cp.classification:cp.classification.InductiveClassifier.alpha]{\crossref{\code{InductiveClassifier.alpha}}}}).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{train}} (\emph{\texttt{Table}}) -- Table of examples used as a training set.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{LOOClassifier (class in cp.classification)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.LOOClassifier}\pysiglinewithargsret{\strong{class }\code{cp.classification.}\bfcode{LOOClassifier}}{\emph{nc\_measure}, \emph{train=None}, \emph{mondrian=False}}{}
Bases: {\hyperref[cp.classification:cp.classification.CrossClassifier]{\crossref{\code{cp.classification.CrossClassifier}}}}

Leave-one-out classifier is a cross conformal classifier with the number of folds equal
to the size of the training set.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{loocp} \PYG{o}{=} \PYG{n}{LOOClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{loocp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.classification.LOOClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.LOOClassifier.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}, \emph{train=None}, \emph{mondrian=False}}{}
\end{fulllineitems}

\index{fit() (cp.classification.LOOClassifier method)}

\begin{fulllineitems}
\phantomsection\label{cp.classification:cp.classification.LOOClassifier.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{train}}{}
\end{fulllineitems}


\end{fulllineitems}



\section{cp.evaluation}
\label{cp.evaluation:cp-evaluation}\label{cp.evaluation:module-cp.evaluation}\label{cp.evaluation::doc}\index{cp.evaluation (module)}
Evaluation module contains methods for evaluation of conformal predictors.

Function {\hyperref[cp.evaluation:cp.evaluation.run]{\crossref{\code{run()}}}} produces Results of an appropriate type by using a Sampler on a given data set
to split it into a training and testing set.

Structure:
\begin{itemize}
\item {} \begin{description}
\item[{Sampler (sampling methods)}] \leavevmode\begin{itemize}
\item {} 
{\hyperref[cp.evaluation:cp.evaluation.RandomSampler]{\crossref{\code{RandomSampler}}}}

\item {} 
{\hyperref[cp.evaluation:cp.evaluation.CrossSampler]{\crossref{\code{CrossSampler}}}}

\item {} 
{\hyperref[cp.evaluation:cp.evaluation.LOOSampler]{\crossref{\code{LOOSampler}}}}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{Results (evaluation results)}] \leavevmode\begin{itemize}
\item {} 
{\hyperref[cp.evaluation:cp.evaluation.ResultsClass]{\crossref{\code{ResultsClass}}}}

\item {} 
{\hyperref[cp.evaluation:cp.evaluation.ResultsRegr]{\crossref{\code{ResultsRegr}}}}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{Evaluation methods}] \leavevmode\begin{itemize}
\item {} 
{\hyperref[cp.evaluation:cp.evaluation.run]{\crossref{\code{run()}}}}

\item {} 
{\hyperref[cp.evaluation:cp.evaluation.run_train_test]{\crossref{\code{run\_train\_test()}}}}

\item {} 
{\hyperref[cp.evaluation:cp.evaluation.calibration_plot]{\crossref{\code{calibration\_plot()}}}}

\end{itemize}

\end{description}

\end{itemize}
\index{Sampler (class in cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Sampler}\pysiglinewithargsret{\strong{class }\code{cp.evaluation.}\bfcode{Sampler}}{\emph{data}}{}
Bases: \code{object}

Base class for various data sampling/splitting methods.
\index{data (cp.evaluation.Sampler attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Sampler.data}\pysigline{\bfcode{data}}
\emph{Table} -- Data set for sampling.

\end{fulllineitems}

\index{n (cp.evaluation.Sampler attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Sampler.n}\pysigline{\bfcode{n}}
\emph{int} -- Size of the data set.

\end{fulllineitems}

\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{s} \PYG{o}{=} \PYG{n}{CrossSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k}{for} \PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o+ow}{in} \PYG{n}{s}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{train}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.evaluation.Sampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Sampler.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{data}}{}
Initialize the data set.

\end{fulllineitems}

\index{\_\_iter\_\_() (cp.evaluation.Sampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Sampler.__iter__}\pysiglinewithargsret{\bfcode{\_\_iter\_\_}}{}{}
\end{fulllineitems}

\index{\_\_next\_\_() (cp.evaluation.Sampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Sampler.__next__}\pysiglinewithargsret{\bfcode{\_\_next\_\_}}{}{}
Extending samplers should implement the \_\_next\_\_ method to return the selected
and remaining part of the data.

\end{fulllineitems}

\index{repeat() (cp.evaluation.Sampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Sampler.repeat}\pysiglinewithargsret{\bfcode{repeat}}{\emph{rep=1}}{}
Repeat sampling several times.

\end{fulllineitems}


\end{fulllineitems}

\index{RandomSampler (class in cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.RandomSampler}\pysiglinewithargsret{\strong{class }\code{cp.evaluation.}\bfcode{RandomSampler}}{\emph{data}, \emph{a}, \emph{b}}{}
Bases: {\hyperref[cp.evaluation:cp.evaluation.Sampler]{\crossref{\code{cp.evaluation.Sampler}}}}

Randomly samples a subset of data in proportion a:b.
\index{k (cp.evaluation.RandomSampler attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.RandomSampler.k}\pysigline{\bfcode{k}}
\emph{float} -- Size of the selected subset.

\end{fulllineitems}

\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{s} \PYG{o}{=} \PYG{n}{RandomSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{s}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.evaluation.RandomSampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.RandomSampler.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{data}, \emph{a}, \emph{b}}{}
Initialize the data set and the size of the desired selection.

\end{fulllineitems}

\index{\_\_iter\_\_() (cp.evaluation.RandomSampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.RandomSampler.__iter__}\pysiglinewithargsret{\bfcode{\_\_iter\_\_}}{}{}
Return a special iterator over a single split of data.

\end{fulllineitems}

\index{\_\_next\_\_() (cp.evaluation.RandomSampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.RandomSampler.__next__}\pysiglinewithargsret{\bfcode{\_\_next\_\_}}{}{}
Splits the data based on a random permutation.

\end{fulllineitems}


\end{fulllineitems}

\index{CrossSampler (class in cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.CrossSampler}\pysiglinewithargsret{\strong{class }\code{cp.evaluation.}\bfcode{CrossSampler}}{\emph{data}, \emph{k}}{}
Bases: {\hyperref[cp.evaluation:cp.evaluation.Sampler]{\crossref{\code{cp.evaluation.Sampler}}}}

Sample the data in {\hyperref[cp.evaluation:cp.evaluation.CrossSampler.k]{\crossref{\code{k}}}} folds. Shuffle the data before determining the folds.
\index{k (cp.evaluation.CrossSampler attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.CrossSampler.k}\pysigline{\bfcode{k}}
\emph{int} -- Number of folds.

\end{fulllineitems}

\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{s} \PYG{o}{=} \PYG{n}{CrossSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k}{for} \PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{train}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.evaluation.CrossSampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.CrossSampler.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{data}, \emph{k}}{}
\end{fulllineitems}

\index{\_\_next\_\_() (cp.evaluation.CrossSampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.CrossSampler.__next__}\pysiglinewithargsret{\bfcode{\_\_next\_\_}}{}{}
Compute the next fold. Initializes a new k-fold split on each repetition of the entire
sampling procedure.

\end{fulllineitems}


\end{fulllineitems}

\index{LOOSampler (class in cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.LOOSampler}\pysiglinewithargsret{\strong{class }\code{cp.evaluation.}\bfcode{LOOSampler}}{\emph{data}}{}
Bases: {\hyperref[cp.evaluation:cp.evaluation.CrossSampler]{\crossref{\code{cp.evaluation.CrossSampler}}}}

Leave-One-Out sampler is a cross sampler with the number of folds equal to the size of the data set.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{s} \PYG{o}{=} \PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k}{for} \PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o+ow}{in} \PYG{n}{s}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{test}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.evaluation.LOOSampler method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.LOOSampler.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{data}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{Results (class in cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Results}\pysigline{\strong{class }\code{cp.evaluation.}\bfcode{Results}}
Bases: \code{object}

Contains results of an evaluation of a conformal predictor
returned by the {\hyperref[cp.evaluation:cp.evaluation.run]{\crossref{\code{run()}}}} function.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{r} \PYG{o}{=} \PYG{n}{run}\PYG{p}{(}\PYG{n}{cp}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{RandomSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{n}{accuracy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.evaluation.Results method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Results.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{}{}
\end{fulllineitems}

\index{add() (cp.evaluation.Results method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Results.add}\pysiglinewithargsret{\bfcode{add}}{\emph{pred}, \emph{ref}}{}
Add a new predicted and corresponding reference value.

\end{fulllineitems}

\index{concatenate() (cp.evaluation.Results method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Results.concatenate}\pysiglinewithargsret{\bfcode{concatenate}}{\emph{r}}{}
Concatenate another set of results.

\end{fulllineitems}

\index{accuracy() (cp.evaluation.Results method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Results.accuracy}\pysiglinewithargsret{\bfcode{accuracy}}{}{}
Compute the accuracy of the predictor averaging verdicts of individual predictions. This is the fraction
of instances that contain the actual/reference class among the predicted ones for classification and the fraction of
instances that contain the actual value within the predicted range for regression.

\end{fulllineitems}

\index{time() (cp.evaluation.Results method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.Results.time}\pysiglinewithargsret{\bfcode{time}}{}{}
\end{fulllineitems}


\end{fulllineitems}

\index{ResultsClass (class in cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass}\pysigline{\strong{class }\code{cp.evaluation.}\bfcode{ResultsClass}}
Bases: {\hyperref[cp.evaluation:cp.evaluation.Results]{\crossref{\code{cp.evaluation.Results}}}}

Results of evaluating a conformal classifier. Provides classification specific efficiency measures.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{r} \PYG{o}{=} \PYG{n}{run}\PYG{p}{(}\PYG{n}{cp}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{RandomSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{n}{singleton\PYGZus{}criterion}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{accuracy() (cp.evaluation.ResultsClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass.accuracy}\pysiglinewithargsret{\bfcode{accuracy}}{\emph{class\_value=None}}{}
Compute accuracy for test instances with a given class value. If this parameter is not given,
compute accuracy over all instances, regardless of their class.

\end{fulllineitems}

\index{confidence() (cp.evaluation.ResultsClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass.confidence}\pysiglinewithargsret{\bfcode{confidence}}{}{}
Average confidence of predictions.

\end{fulllineitems}

\index{credibility() (cp.evaluation.ResultsClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass.credibility}\pysiglinewithargsret{\bfcode{credibility}}{}{}
Average credibility of predictions.

\end{fulllineitems}

\index{confusion() (cp.evaluation.ResultsClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass.confusion}\pysiglinewithargsret{\bfcode{confusion}}{\emph{actual}, \emph{predicted}}{}
Compute the number of singleton predictions of class \titleref{predicted} when the actual class is \titleref{actual}.
\paragraph{Examples}

Drawing a confusion matrix.

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{data} \PYG{o}{=} \PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{r} \PYG{o}{=} \PYG{n}{run}\PYG{p}{(}\PYG{n}{cp}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{RandomSampler}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{values} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{domain}\PYG{o}{.}\PYG{n}{class\PYGZus{}var}\PYG{o}{.}\PYG{n}{values}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{form} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}: \PYGZgt{}20\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{*}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{values}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{form}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actual}\PYG{l+s+s1}{\PYGZbs{}}\PYG{l+s+s1}{predicted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{o}{*}\PYG{n}{values}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k}{for} \PYG{n}{a} \PYG{o+ow}{in} \PYG{n}{values}\PYG{p}{:}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n}{c} \PYG{o}{=} \PYG{p}{[}\PYG{n}{r}\PYG{o}{.}\PYG{n}{confusion}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)} \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{values}\PYG{p}{]}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }    \PYG{n+nb}{print}\PYG{p}{(}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}: \PYGZgt{}20\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{*}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{c}\PYG{p}{)}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{o}{*}\PYG{n}{c}\PYG{p}{)}\PYG{p}{)}
\PYG{g+go}{    actual\PYGZbs{}predicted         Iris\PYGZhy{}setosa     Iris\PYGZhy{}versicolor      Iris\PYGZhy{}virginica}
\PYG{g+go}{         Iris\PYGZhy{}setosa                  18                   0                   0}
\PYG{g+go}{     Iris\PYGZhy{}versicolor                   0                  14                   4}
\PYG{g+go}{      Iris\PYGZhy{}virginica                   0                   0                  12}
\end{Verbatim}

\end{fulllineitems}

\index{multiple\_criterion() (cp.evaluation.ResultsClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass.multiple_criterion}\pysiglinewithargsret{\bfcode{multiple\_criterion}}{}{}
Number of cases with multiple predicted classes.

\end{fulllineitems}

\index{singleton\_criterion() (cp.evaluation.ResultsClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass.singleton_criterion}\pysiglinewithargsret{\bfcode{singleton\_criterion}}{}{}
Number of cases with a single predicted class.

\end{fulllineitems}

\index{empty\_criterion() (cp.evaluation.ResultsClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass.empty_criterion}\pysiglinewithargsret{\bfcode{empty\_criterion}}{}{}
Number of cases with no predicted classes.

\end{fulllineitems}

\index{singleton\_correct() (cp.evaluation.ResultsClass method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsClass.singleton_correct}\pysiglinewithargsret{\bfcode{singleton\_correct}}{}{}
Fraction of singleton predictions that are correct.

\end{fulllineitems}


\end{fulllineitems}

\index{ResultsRegr (class in cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsRegr}\pysigline{\strong{class }\code{cp.evaluation.}\bfcode{ResultsRegr}}
Bases: {\hyperref[cp.evaluation:cp.evaluation.Results]{\crossref{\code{cp.evaluation.Results}}}}

Results of evaluating a conformal regressor. Provides regression specific efficiency measures.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ir} \PYG{o}{=} \PYG{n}{InductiveRegressor}\PYG{p}{(}\PYG{n}{AbsErrorKNN}\PYG{p}{(}\PYG{n}{Euclidean}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{average}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{r} \PYG{o}{=} \PYG{n}{run}\PYG{p}{(}\PYG{n}{ir}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{RandomSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{n}{interdecile\PYGZus{}range}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{widths() (cp.evaluation.ResultsRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsRegr.widths}\pysiglinewithargsret{\bfcode{widths}}{}{}
\end{fulllineitems}

\index{median\_range() (cp.evaluation.ResultsRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsRegr.median_range}\pysiglinewithargsret{\bfcode{median\_range}}{}{}
Median width of predicted ranges.

\end{fulllineitems}

\index{mean\_range() (cp.evaluation.ResultsRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsRegr.mean_range}\pysiglinewithargsret{\bfcode{mean\_range}}{}{}
Mean width of predicted ranges.

\end{fulllineitems}

\index{std\_dev() (cp.evaluation.ResultsRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsRegr.std_dev}\pysiglinewithargsret{\bfcode{std\_dev}}{}{}
Standard deviation of widths of predicted ranges.

\end{fulllineitems}

\index{interdecile\_range() (cp.evaluation.ResultsRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsRegr.interdecile_range}\pysiglinewithargsret{\bfcode{interdecile\_range}}{}{}
Difference between the first and ninth decile of widths of predicted ranges.

\end{fulllineitems}

\index{interdecile\_mean() (cp.evaluation.ResultsRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.ResultsRegr.interdecile_mean}\pysiglinewithargsret{\bfcode{interdecile\_mean}}{}{}
Mean width discarding the smallest and largest 10\% of widths of predicted ranges.

\end{fulllineitems}


\end{fulllineitems}

\index{run() (in module cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.run}\pysiglinewithargsret{\code{cp.evaluation.}\bfcode{run}}{\emph{cp}, \emph{eps}, \emph{sampler}, \emph{rep=1}}{}
Run method is used to repeat an experiment one or more times with different splits of the dataset
into a training and testing set. The splits are defined by the provided sampler. The conformal predictor
itself might further split the testing set internally for its computations (e.g. inductive or cross predictors).

Run the conformal predictor \titleref{cp} on the datasets defined by the provided sampler and number of repetitions
and construct the results. Fit the conformal predictor on each training set returned by the sampler and
evaluate it on the corresponding test set.
Inductive conformal predictors use one third of the training set (random subset) for calibration.

For more control over the exact datasets used for training, testing and calibration see {\hyperref[cp.evaluation:cp.evaluation.run_train_test]{\crossref{\code{run\_train\_test()}}}}.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
{\hyperref[cp.evaluation:cp.evaluation.ResultsClass]{\crossref{\code{ResultsClass}}}} or {\hyperref[cp.evaluation:cp.evaluation.ResultsRegr]{\crossref{\code{ResultsRegr}}}}

\end{description}\end{quote}
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{r} \PYG{o}{=} \PYG{n}{run}\PYG{p}{(}\PYG{n}{cp}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{CrossSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,} \PYG{n}{rep}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{n}{accuracy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{r}\PYG{o}{.}\PYG{n}{empty\PYGZus{}criterion}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

The above example uses a {\hyperref[cp.evaluation:cp.evaluation.CrossSampler]{\crossref{\code{CrossSampler}}}} to define training and testing datasets. Each fold is used as the test
set and the rest as a training set. The entire process is repeated three times with different fold splits
and results in 3*n predictions, where n is the size of the dataset.

\end{fulllineitems}

\index{run\_train\_test() (in module cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.run_train_test}\pysiglinewithargsret{\code{cp.evaluation.}\bfcode{run\_train\_test}}{\emph{cp}, \emph{eps}, \emph{train}, \emph{test}, \emph{calibrate=None}}{}
Fits the conformal predictor \titleref{cp} on the training dataset and evaluates it on the testing set.
Inductive conformal predictors use the provided calibration set or default to extracting one third
of the training set (random subset) for calibration.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
{\hyperref[cp.evaluation:cp.evaluation.ResultsClass]{\crossref{\code{ResultsClass}}}} or {\hyperref[cp.evaluation:cp.evaluation.ResultsRegr]{\crossref{\code{ResultsRegr}}}}

\end{description}\end{quote}
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{tab} \PYG{o}{=} \PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{r} \PYG{o}{=} \PYG{n}{run\PYGZus{}train\PYGZus{}test}\PYG{p}{(}\PYG{n}{cp}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{tab}\PYG{p}{[}\PYG{p}{:}\PYG{l+m+mi}{100}\PYG{p}{]}\PYG{p}{,} \PYG{n}{tab}\PYG{p}{[}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{n}{accuracy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{r}\PYG{o}{.}\PYG{n}{singleton\PYGZus{}criterion}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

\end{fulllineitems}

\index{calibration\_plot() (in module cp.evaluation)}

\begin{fulllineitems}
\phantomsection\label{cp.evaluation:cp.evaluation.calibration_plot}\pysiglinewithargsret{\code{cp.evaluation.}\bfcode{calibration\_plot}}{\emph{cp}, \emph{data}, \emph{k=11}, \emph{rep=1}, \emph{title='calibration plot'}, \emph{fname='cplot.png'}}{}
Draw and save a calibration plot by evaluating the conformal predictor (\titleref{cp})
with different significance values \titleref{eps} on random train/test splits.
Repeat the experiment \titleref{rep} times.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{calibration\PYGZus{}plot}\PYG{p}{(}\PYG{n}{cp}\PYG{p}{,} \PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}

\end{fulllineitems}



\section{cp.nonconformity}
\label{cp.nonconformity:cp-nonconformity}\label{cp.nonconformity:module-cp.nonconformity}\label{cp.nonconformity::doc}\index{cp.nonconformity (module)}
Nonconformity module contains nonconformity scores for classification and regression.

Structure:
\begin{itemize}
\item {} \begin{description}
\item[{ClassNC (classification scores)}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{ClassModelNC (model based)}] \leavevmode
{\hyperref[cp.nonconformity:cp.nonconformity.InverseProbability]{\crossref{\code{InverseProbability}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.ProbabilityMargin]{\crossref{\code{ProbabilityMargin}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.SVMDistance]{\crossref{\code{SVMDistance}}}},
{\hyperref[cp.nonconformity:cp.nonconformity.LOOClassNC]{\crossref{\code{LOOClassNC}}}}

\end{description}

\item {} \begin{description}
\item[{ClassNearestNeighboursNC (nearest neighbours based)}] \leavevmode
{\hyperref[cp.nonconformity:cp.nonconformity.KNNDistance]{\crossref{\code{KNNDistance}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.KNNFraction]{\crossref{\code{KNNFraction}}}}

\end{description}

\end{itemize}

\end{description}

\item {} \begin{description}
\item[{RegrNC (regression scores)}] \leavevmode\begin{itemize}
\item {} \begin{description}
\item[{RegrModelNC (model based)}] \leavevmode
{\hyperref[cp.nonconformity:cp.nonconformity.AbsError]{\crossref{\code{AbsError}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.AbsErrorRF]{\crossref{\code{AbsErrorRF}}}} {\hyperref[cp.nonconformity:cp.nonconformity.AbsErrorNormalized]{\crossref{\code{AbsErrorNormalized}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.LOORegrNC]{\crossref{\code{LOORegrNC}}}},
{\hyperref[cp.nonconformity:cp.nonconformity.ErrorModelNC]{\crossref{\code{ErrorModelNC}}}}

\end{description}

\item {} \begin{description}
\item[{RegrNearestNeighboursNC (nearest neighbours based)}] \leavevmode
{\hyperref[cp.nonconformity:cp.nonconformity.AbsErrorKNN]{\crossref{\code{AbsErrorKNN}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.AvgErrorKNN]{\crossref{\code{AvgErrorKNN}}}}

\end{description}

\end{itemize}

\end{description}

\end{itemize}
\index{ClassNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassNC}\pysigline{\strong{class }\code{cp.nonconformity.}\bfcode{ClassNC}}
Bases: \code{object}

Base class for classification nonconformity scores.

Extending classes should implement {\hyperref[cp.nonconformity:cp.nonconformity.ClassNC.fit]{\crossref{\code{fit()}}}} and {\hyperref[cp.nonconformity:cp.nonconformity.ClassNC.nonconformity]{\crossref{\code{nonconformity()}}}} methods.
\index{fit() (cp.nonconformity.ClassNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassNC.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Process the data used for later calculation of nonconformities.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{data}} (\emph{\texttt{Table}}) -- Data set.

\end{description}\end{quote}

\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.ClassNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassNC.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
Compute the nonconformity score of the given \titleref{instance}.

\end{fulllineitems}


\end{fulllineitems}

\index{ClassModelNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassModelNC}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{ClassModelNC}}{\emph{classifier}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.ClassNC]{\crossref{\code{cp.nonconformity.ClassNC}}}}

Base class for classification nonconformity scores that are based on an underlying classifier.

Extending classes should implement {\hyperref[cp.nonconformity:cp.nonconformity.ClassNC.nonconformity]{\crossref{\code{ClassNC.nonconformity()}}}} method.
\index{learner (cp.nonconformity.ClassModelNC attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassModelNC.learner}\pysigline{\bfcode{learner}}
Untrained underlying classifier.

\end{fulllineitems}

\index{model (cp.nonconformity.ClassModelNC attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassModelNC.model}\pysigline{\bfcode{model}}
Trained underlying classifier.

\end{fulllineitems}

\index{\_\_init\_\_() (cp.nonconformity.ClassModelNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassModelNC.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{classifier}}{}
Store the provided classifier as {\hyperref[cp.nonconformity:cp.nonconformity.ClassModelNC.learner]{\crossref{\code{learner}}}}.

\end{fulllineitems}

\index{fit() (cp.nonconformity.ClassModelNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassModelNC.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Train the underlying classifier on provided data and store the trained model.

\end{fulllineitems}


\end{fulllineitems}

\index{InverseProbability (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.InverseProbability}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{InverseProbability}}{\emph{classifier}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.ClassModelNC]{\crossref{\code{cp.nonconformity.ClassModelNC}}}}

Inverse probability nonconformity score returns \(1 - p\), where \(p\) is the probability
assigned to the actual class by the underlying classification model ({\hyperref[cp.nonconformity:cp.nonconformity.ClassModelNC.model]{\crossref{\code{ClassModelNC.model}}}}).
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{tp} \PYG{o}{=} \PYG{n}{TransductiveClassifier}\PYG{p}{(}\PYG{n}{InverseProbability}\PYG{p}{(}\PYG{n}{NaiveBayesLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{nonconformity() (cp.nonconformity.InverseProbability method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.InverseProbability.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{ProbabilityMargin (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ProbabilityMargin}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{ProbabilityMargin}}{\emph{classifier}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.ClassModelNC]{\crossref{\code{cp.nonconformity.ClassModelNC}}}}

Probability margin nonconformity score measures the difference \(d_p\) between the predicted probability
of the actual class and the largest probability corresponding to some other class. To put the values on scale
from 0 to 1, the nonconformity function returns \((1 - d_p) / 2\).
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{tp} \PYG{o}{=} \PYG{n}{TransductiveClassifier}\PYG{p}{(}\PYG{n}{ProbabilityMargin}\PYG{p}{(}\PYG{n}{LogisticRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{tp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{nonconformity() (cp.nonconformity.ProbabilityMargin method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ProbabilityMargin.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{SVMDistance (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.SVMDistance}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{SVMDistance}}{\emph{classifier}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.ClassNC]{\crossref{\code{cp.nonconformity.ClassNC}}}}

SVMDistance nonconformity score measures the distance from the SVM's decision boundary. The score depends
on the distance and the side of the decision boundary that the example lies on.
Examples that lie on the correct side of the decision boundary and would therefore result in a
correct prediction using the SVM classifier have a nonconformity score less than 1, while the incorrectly
predicted examples have a score more than 1.
\begin{equation*}
\begin{split}\mathit{nc} =
\begin{cases}
\frac{1}{1+d} & \text{correct}\\
1+d &\text{incorrect}
\end{cases}\end{split}
\end{equation*}
The provided SVM classifier must be a sklearn's SVM classifier (SVC, LinearSVC, NuSVC)
providing the decision\_function() which computes the distance to decision boundary. This nonconformity works
only for binary classification problems.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{svm} \PYG{k}{import} \PYG{n}{SVC}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{titanic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{calibrate} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{RandomSampler}\PYG{p}{(}\PYG{n}{train}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{icp} \PYG{o}{=} \PYG{n}{InductiveClassifier}\PYG{p}{(}\PYG{n}{SVMDistance}\PYG{p}{(}\PYG{n}{SVC}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{,} \PYG{n}{calibrate}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{icp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.nonconformity.SVMDistance method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.SVMDistance.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{classifier}}{}
\end{fulllineitems}

\index{fit() (cp.nonconformity.SVMDistance method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.SVMDistance.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.SVMDistance method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.SVMDistance.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{NearestNeighbours (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.NearestNeighbours}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{NearestNeighbours}}{\emph{distance}, \emph{k=1}}{}
Bases: \code{object}

Base class for nonconformity measures based on nearest neighbours.
\index{distance (cp.nonconformity.NearestNeighbours attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.NearestNeighbours.distance}\pysigline{\bfcode{distance}}
Distance measure.

\end{fulllineitems}

\index{k (cp.nonconformity.NearestNeighbours attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.NearestNeighbours.k}\pysigline{\bfcode{k}}
\emph{int} -- Number of nearest neighbours.

\end{fulllineitems}

\index{\_\_init\_\_() (cp.nonconformity.NearestNeighbours method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.NearestNeighbours.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{distance}, \emph{k=1}}{}
Store the distance measure and the number of neighbours.

\end{fulllineitems}

\index{fit() (cp.nonconformity.NearestNeighbours method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.NearestNeighbours.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Store the data for finding nearest neighbours.

\end{fulllineitems}

\index{neighbours() (cp.nonconformity.NearestNeighbours method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.NearestNeighbours.neighbours}\pysiglinewithargsret{\bfcode{neighbours}}{\emph{instance}}{}
Compute distances to all other data instances using the distance measure ({\hyperref[cp.nonconformity:cp.nonconformity.NearestNeighbours.distance]{\crossref{\code{distance}}}}).

Excludes data instances that are equal to the provided \titleref{instance}.
\begin{quote}\begin{description}
\item[{Returns}] \leavevmode
List of pairs (distance, instance) in increasing order of distances.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{ClassNearestNeighboursNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ClassNearestNeighboursNC}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{ClassNearestNeighboursNC}}{\emph{distance}, \emph{k=1}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.NearestNeighbours]{\crossref{\code{cp.nonconformity.NearestNeighbours}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.ClassNC]{\crossref{\code{cp.nonconformity.ClassNC}}}}

Base class for nearest neighbrours based classification nonconformity scores.

\end{fulllineitems}

\index{KNNDistance (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.KNNDistance}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{KNNDistance}}{\emph{distance}, \emph{k=1}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.ClassNearestNeighboursNC]{\crossref{\code{cp.nonconformity.ClassNearestNeighboursNC}}}}

Computes the sum of distances to k nearest neighbours of the same class as the given instance and
the sum of distances to k nearest neighbours of other classes. Returns their ratio.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{Orange}\PYG{n+nn}{.}\PYG{n+nn}{distance} \PYG{k}{import} \PYG{n}{Euclidean}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{KNNDistance}\PYG{p}{(}\PYG{n}{Euclidean}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{cp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{nonconformity() (cp.nonconformity.KNNDistance method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.KNNDistance.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{KNNFraction (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.KNNFraction}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{KNNFraction}}{\emph{distance}, \emph{k=1}, \emph{weighted=False}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.ClassNearestNeighboursNC]{\crossref{\code{cp.nonconformity.ClassNearestNeighboursNC}}}}

Computes the k nearest neighbours of the given instance. Returns the fraction of instances of the
same class as the given instance within its k nearest neighbours.

Weighted version uses weights \(1/d_i\) based on distances instead of simply counting the instances.
Non-weighted version is equivalent to using a value 1 for all weights.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{iris}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cp} \PYG{o}{=} \PYG{n}{CrossClassifier}\PYG{p}{(}\PYG{n}{KNNFraction}\PYG{p}{(}\PYG{n}{Euclidean}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{weighted}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{cp}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.nonconformity.KNNFraction method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.KNNFraction.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{distance}, \emph{k=1}, \emph{weighted=False}}{}
\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.KNNFraction method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.KNNFraction.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{LOOClassNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOOClassNC}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{LOOClassNC}}{\emph{classifier}, \emph{distance}, \emph{k}, \emph{relative=True}, \emph{include=False}, \emph{neighbourhood='fixed'}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.NearestNeighbours]{\crossref{\code{cp.nonconformity.NearestNeighbours}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.ClassNC]{\crossref{\code{cp.nonconformity.ClassNC}}}}
\begin{equation*}
\begin{split}\mathit{nc} = \mathit{error} + (1 - p)
\quad \text{or} \quad
\mathit{nc} = \frac{1 - p}{\mathit{error}}\end{split}
\end{equation*}
\(p\) ... probability of actual class predicted from \(N_k(z^*)\) - k nearest neighbours
of the instance \(z^*\)

The first nonconformity score is used when the parameter \code{relative} is set to \emph{False}
and the second one when it is set to \emph{True}.
\begin{equation*}
\begin{split}\mathit{error} = \frac {\sum_{z_i \in N_k(z^*)} w_i (1 - p_i)} {\sum_{z_i \in N_k(z^*)} w_i},
\quad w_i = \frac{1}{d(x^*, x_i)}\end{split}
\end{equation*}
\(p_i\) ... probability of actual class predicted from \(N_k(z') \setminus z_i\) or
\(N_k(z') \setminus z_i \cup z^*\) if the parameter \code{include} is set to \emph{True}. \(z'\) is
\(z^*\) if the \code{neighbourhood} parameter is `\emph{fixed}` and \(z_i\) if it's `\emph{variable}`.
\index{\_\_init\_\_() (cp.nonconformity.LOOClassNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOOClassNC.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{classifier}, \emph{distance}, \emph{k}, \emph{relative=True}, \emph{include=False}, \emph{neighbourhood='fixed'}}{}
Initialize the parameters.

\end{fulllineitems}

\index{fit() (cp.nonconformity.LOOClassNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOOClassNC.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Store the data for finding nearest neighbours and initialize cache.

\end{fulllineitems}

\index{get\_neighbourhood() (cp.nonconformity.LOOClassNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOOClassNC.get_neighbourhood}\pysiglinewithargsret{\bfcode{get\_neighbourhood}}{\emph{inst}}{}
Construct an Orange data Table consisting of instance's k nearest neighbours.
Cache the results for later calls with the same instance.

\end{fulllineitems}

\index{error() (cp.nonconformity.LOOClassNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOOClassNC.error}\pysiglinewithargsret{\bfcode{error}}{\emph{inst}, \emph{neighbours}}{}
Compute the average weighted probability prediction error for predicting the actual class
of each neighbour from the other ones. Include the new example among the neighbours
if the parameter \code{include} is True.

\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.LOOClassNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOOClassNC.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{inst}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{RegrNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrNC}\pysigline{\strong{class }\code{cp.nonconformity.}\bfcode{RegrNC}}
Bases: \code{object}

Base class for regression nonconformity scores.

Extending classes should implement {\hyperref[cp.nonconformity:cp.nonconformity.RegrNC.fit]{\crossref{\code{fit()}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.RegrNC.nonconformity]{\crossref{\code{nonconformity()}}}} and {\hyperref[cp.nonconformity:cp.nonconformity.RegrNC.predict]{\crossref{\code{predict()}}}} methods.
\index{fit() (cp.nonconformity.RegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrNC.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Process the data used for later calculation of nonconformities.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{data}} (\emph{\texttt{Table}}) -- Data set.

\end{description}\end{quote}

\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.RegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrNC.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
Compute the nonconformity score of the given \titleref{instance}.

\end{fulllineitems}

\index{predict() (cp.nonconformity.RegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrNC.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
Compute the inverse of the nonconformity score. Determine a range of values for which the
nonconformity of the given \titleref{instance} does not exceed \titleref{nc}.

\end{fulllineitems}


\end{fulllineitems}

\index{RegrModelNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrModelNC}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{RegrModelNC}}{\emph{classifier}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.RegrNC]{\crossref{\code{cp.nonconformity.RegrNC}}}}

Base class for regression nonconformity scores that are based on an underlying classifier.

Extending classes should implement {\hyperref[cp.nonconformity:cp.nonconformity.RegrNC.nonconformity]{\crossref{\code{RegrNC.nonconformity()}}}} and {\hyperref[cp.nonconformity:cp.nonconformity.RegrNC.predict]{\crossref{\code{RegrNC.predict()}}}} methods.
\index{learner (cp.nonconformity.RegrModelNC attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrModelNC.learner}\pysigline{\bfcode{learner}}
Untrained underlying classifier.

\end{fulllineitems}

\index{model (cp.nonconformity.RegrModelNC attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrModelNC.model}\pysigline{\bfcode{model}}
Trained underlying classifier.

\end{fulllineitems}

\index{\_\_init\_\_() (cp.nonconformity.RegrModelNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrModelNC.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{classifier}}{}
Store the provided classifier as {\hyperref[cp.nonconformity:cp.nonconformity.RegrModelNC.learner]{\crossref{\code{learner}}}}.

\end{fulllineitems}

\index{fit() (cp.nonconformity.RegrModelNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrModelNC.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Train the underlying classifier on provided data and store the trained model.

\end{fulllineitems}


\end{fulllineitems}

\index{AbsError (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsError}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{AbsError}}{\emph{classifier}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.RegrModelNC]{\crossref{\code{cp.nonconformity.RegrModelNC}}}}

Absolute error nonconformity score returns the absolute difference between
the predicted value (\(\hat{y}\)) by the underlying {\hyperref[cp.nonconformity:cp.nonconformity.RegrModelNC.model]{\crossref{\code{RegrModelNC.model}}}}
and the actual value (\(y^{*}\)).
\begin{equation*}
\begin{split}\mathit{nc} = |\hat{y}-y^{*}|\end{split}
\end{equation*}\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cr} \PYG{o}{=} \PYG{n}{CrossRegressor}\PYG{p}{(}\PYG{n}{AbsError}\PYG{p}{(}\PYG{n}{LinearRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{cr}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{nonconformity() (cp.nonconformity.AbsError method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsError.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
\end{fulllineitems}

\index{predict() (cp.nonconformity.AbsError method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsError.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{AbsErrorRF (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorRF}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{AbsErrorRF}}{\emph{classifier}, \emph{rf}, \emph{beta=0.5}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.RegrModelNC]{\crossref{\code{cp.nonconformity.RegrModelNC}}}}

AbsErrorRF is based on an underlying regressor and a random forest. The prediction errors of regressor
are used as nonconformity scores and are normalized by the standard deviation of predictions coming from
individual trees in the forest.
\begin{equation*}
\begin{split}\mathit{nc} = \frac{|\hat{y}-y^{*}|}{\sigma_\mathit{RF} + \beta}\end{split}
\end{equation*}\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k}{import} \PYG{n}{RandomForestRegressor}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{icr} \PYG{o}{=} \PYG{n}{InductiveRegressor}\PYG{p}{(}\PYG{n}{AbsErrorRF}\PYG{p}{(}\PYG{n}{RandomForestRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{RandomForestRegressor}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{r} \PYG{o}{=} \PYG{n}{run}\PYG{p}{(}\PYG{n}{icr}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{CrossSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{n}{accuracy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{r}\PYG{o}{.}\PYG{n}{median\PYGZus{}range}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{r}\PYG{o}{.}\PYG{n}{interdecile\PYGZus{}mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.nonconformity.AbsErrorRF method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorRF.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{classifier}, \emph{rf}, \emph{beta=0.5}}{}
Store the classifier and beta parameter.

\end{fulllineitems}

\index{fit() (cp.nonconformity.AbsErrorRF method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorRF.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Train the underlying classifier on provided data and store the trained model.

\end{fulllineitems}

\index{norm() (cp.nonconformity.AbsErrorRF method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorRF.norm}\pysiglinewithargsret{\bfcode{norm}}{\emph{inst}}{}
Normalization factor is equal to the standard deviation of predictions from trees in a random forest
plus a constant term \code{beta}.

\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.AbsErrorRF method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorRF.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{inst}}{}
\end{fulllineitems}

\index{predict() (cp.nonconformity.AbsErrorRF method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorRF.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{ErrorModelNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ErrorModelNC}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{ErrorModelNC}}{\emph{classifier}, \emph{error\_classifier}, \emph{beta=0.5}, \emph{loo=False}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.RegrModelNC]{\crossref{\code{cp.nonconformity.RegrModelNC}}}}

ErrorModelNC is based on two underlying regressors. The first one is trained to predict the value while the
second one is used for predicting logarithms of the errors made by the first one.

H. Papadopoulos and H. Haralambous. \emph{Reliable prediction intervals with regression neural networks}.
Neural Networks (2011).
\begin{equation*}
\begin{split}\mathit{nc} = \frac{|\hat{y}-y^{*}|}{\exp(\mu)-1 + \beta}\end{split}
\end{equation*}
\(\mu\) ... prediction for the value of \(\log(|\hat{y}-y^{*}|+1)\) returned by the second regressor

Parameter \code{loo} determines whether to use a leave-one-out schema for building the training set
of errors for the second regressor or not.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{nc} \PYG{o}{=} \PYG{n}{ErrorModelNC}\PYG{p}{(}\PYG{n}{SVRLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{LinearRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{icr} \PYG{o}{=} \PYG{n}{InductiveRegressor}\PYG{p}{(}\PYG{n}{nc}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{r} \PYG{o}{=} \PYG{n}{run}\PYG{p}{(}\PYG{n}{icr}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{CrossSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{r}\PYG{o}{.}\PYG{n}{accuracy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{r}\PYG{o}{.}\PYG{n}{median\PYGZus{}range}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{r}\PYG{o}{.}\PYG{n}{interdecile\PYGZus{}mean}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.nonconformity.ErrorModelNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ErrorModelNC.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{classifier}, \emph{error\_classifier}, \emph{beta=0.5}, \emph{loo=False}}{}
\end{fulllineitems}

\index{fit() (cp.nonconformity.ErrorModelNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ErrorModelNC.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.ErrorModelNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ErrorModelNC.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{inst}}{}
\end{fulllineitems}

\index{predict() (cp.nonconformity.ErrorModelNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ErrorModelNC.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{ExperimentalNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ExperimentalNC}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{ExperimentalNC}}{\emph{rf}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.RegrModelNC]{\crossref{\code{cp.nonconformity.RegrModelNC}}}}
\index{\_\_init\_\_() (cp.nonconformity.ExperimentalNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ExperimentalNC.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{rf}}{}
\end{fulllineitems}

\index{fit() (cp.nonconformity.ExperimentalNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ExperimentalNC.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
\end{fulllineitems}

\index{norm() (cp.nonconformity.ExperimentalNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ExperimentalNC.norm}\pysiglinewithargsret{\bfcode{norm}}{\emph{inst}}{}
\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.ExperimentalNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ExperimentalNC.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{inst}}{}
\end{fulllineitems}

\index{predict() (cp.nonconformity.ExperimentalNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.ExperimentalNC.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{AbsErrorNormalized (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{AbsErrorNormalized}}{\emph{classifier}, \emph{distance}, \emph{k}, \emph{gamma=0.5}, \emph{rho=0.5}, \emph{exp=True}, \emph{rf=None}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.RegrModelNC]{\crossref{\code{cp.nonconformity.RegrModelNC}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.NearestNeighbours]{\crossref{\code{cp.nonconformity.NearestNeighbours}}}}

Normalized absolute error prediction uses an underlying regression model to predict the value,
which is then normalized by the distance and variance of the nearest neighbours.

H. Papadopoulos, V. Vovk and A. Gammerman. \emph{Regression Conformal Prediction with Nearest Neighbours}.
Journal of Artificial Intelligence Research (2011).
\begin{equation*}
\begin{split}\mathit{nc} = \frac{|\hat{y}-y^{*}|}{\exp(\gamma \lambda^*) + \exp(\rho \xi^*)}
\quad \text{or} \quad
\mathit{nc} = \frac{|\hat{y}-y^{*}|}{\gamma + \lambda^* + \xi^*}\end{split}
\end{equation*}
The first nonconformity score is used when the parameter \code{exp} is set to \emph{True}
and the second one when it is set to \emph{False}.
\begin{equation*}
\begin{split}\lambda^* = \frac{d_k(z^*)}{\mathit{median}(\{d_k(z), z \in T\})}, \quad
d_k(z) = \sum_{z_i \in N_k(z)} distance(x, x_i)\end{split}
\end{equation*}\begin{equation*}
\begin{split}\xi^* = \frac{\sigma_k(z^*)}{\mathit{median}(\{\sigma_k(z), z \in T\})}, \quad
\sigma_k(z) = \sqrt{\frac{1}{k} \sum_{z_i \in N_k(z)}(y_i-\bar{y})}\end{split}
\end{equation*}
Parameter \code{rf} enables the use of a random forest for computing the standard deviation
of predictions instead of the nearest neighbours.
\index{\_\_init\_\_() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{classifier}, \emph{distance}, \emph{k}, \emph{gamma=0.5}, \emph{rho=0.5}, \emph{exp=True}, \emph{rf=None}}{}
Initialize the parameters.

\end{fulllineitems}

\index{fit() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Train the underlying model and precompute medians for nonconformity scores.

\end{fulllineitems}

\index{\_d() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized._d}\pysiglinewithargsret{\bfcode{\_d}}{\emph{inst}}{}
Sum of distances to nearest neighbours.

\end{fulllineitems}

\index{\_lambda() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized._lambda}\pysiglinewithargsret{\bfcode{\_lambda}}{\emph{inst}}{}
Normalized distance measure.

\end{fulllineitems}

\index{\_sigma() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized._sigma}\pysiglinewithargsret{\bfcode{\_sigma}}{\emph{inst}}{}
Standard deviation of y values. This comes either from the nearest neighbours or from the
predictions of individual trees in a random forest if the \code{rf} is provided.

\end{fulllineitems}

\index{\_xi() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized._xi}\pysiglinewithargsret{\bfcode{\_xi}}{\emph{inst}}{}
Normalized variance measure.

\end{fulllineitems}

\index{norm() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized.norm}\pysiglinewithargsret{\bfcode{norm}}{\emph{inst}}{}
Compute the normalization factor.

\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{inst}}{}
\end{fulllineitems}

\index{predict() (cp.nonconformity.AbsErrorNormalized method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorNormalized.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{LOORegrNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOORegrNC}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{LOORegrNC}}{\emph{classifier}, \emph{distance}, \emph{k}, \emph{relative=True}, \emph{include=False}, \emph{neighbourhood='fixed'}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.NearestNeighbours]{\crossref{\code{cp.nonconformity.NearestNeighbours}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.RegrNC]{\crossref{\code{cp.nonconformity.RegrNC}}}}
\begin{equation*}
\begin{split}\mathit{nc} = \mathit{error} + |\hat{y}-y^{*}|
\quad \text{or} \quad
\mathit{nc} = \frac{|\hat{y}-y^{*}|}{\mathit{error}}\end{split}
\end{equation*}
\(\hat{y}\) ... value predicted from \(N_k(z^*)\)

The first nonconformity score is used when the parameter \code{relative} is set to \emph{False}
and the second one when it is set to \emph{True}.
\begin{equation*}
\begin{split}\mathit{error} = \frac {\sum_{z_i \in N_k(z^*)} w_i |\hat{y_i}-y_i|} {\sum_{z_i \in N_k(z^*)} w_i},
\quad w_i = \frac{1}{d(x^*, x_i)}\end{split}
\end{equation*}
\(\hat{y_i}\) ... value predicted from \(N_k(z') \setminus z_i\) or
\(N_k(z') \setminus z_i \cup z^*\) if the parameter \code{include} is set to \emph{True}. \(z'\) is
\(z^*\) if the \code{neighbourhood} parameter is `\emph{fixed}` and \(z_i\) if it's `\emph{variable}`.
\index{\_\_init\_\_() (cp.nonconformity.LOORegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOORegrNC.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{classifier}, \emph{distance}, \emph{k}, \emph{relative=True}, \emph{include=False}, \emph{neighbourhood='fixed'}}{}
Initialize the parameters.

\end{fulllineitems}

\index{fit() (cp.nonconformity.LOORegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOORegrNC.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{data}}{}
Store the data for finding nearest neighbours and initialize cache.

\end{fulllineitems}

\index{get\_neighbourhood() (cp.nonconformity.LOORegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOORegrNC.get_neighbourhood}\pysiglinewithargsret{\bfcode{get\_neighbourhood}}{\emph{inst}}{}
Construct an Orange data Table consisting of instance's k nearest neighbours.
Cache the results for later calls with the same instance.

\end{fulllineitems}

\index{error() (cp.nonconformity.LOORegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOORegrNC.error}\pysiglinewithargsret{\bfcode{error}}{\emph{inst}, \emph{neighbours}}{}
Compute the average weighted error for predicting the value of each neighbour from the other ones.
Include the new example among the neighbours if the parameter \code{include} is True.

\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.LOORegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOORegrNC.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{inst}}{}
\end{fulllineitems}

\index{predict() (cp.nonconformity.LOORegrNC method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.LOORegrNC.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{RegrNearestNeighboursNC (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.RegrNearestNeighboursNC}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{RegrNearestNeighboursNC}}{\emph{distance}, \emph{k=1}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.NearestNeighbours]{\crossref{\code{cp.nonconformity.NearestNeighbours}}}}, {\hyperref[cp.nonconformity:cp.nonconformity.RegrNC]{\crossref{\code{cp.nonconformity.RegrNC}}}}

Base class for nearest neighbours based regression nonconformity scores.

\end{fulllineitems}

\index{AbsErrorKNN (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorKNN}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{AbsErrorKNN}}{\emph{distance}, \emph{k}, \emph{average=False}, \emph{variance=False}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.RegrNearestNeighboursNC]{\crossref{\code{cp.nonconformity.RegrNearestNeighboursNC}}}}

Absolute error of k nearest neighbours computes the average value of the k nearest neighbours and
returns an absolute difference between this average (\(y_k\)) and the actual value (\(y^{*}\)).
\begin{equation*}
\begin{split}\bar{y} &= 1/k \sum_{N_k(x^{*})} y_i \\
\mathit{nc} &= |\bar{y} - y^{*}|\end{split}
\end{equation*}
Weighted version can normalize by average and/or variance.
\begin{equation*}
\begin{split}\mathit{nc} = \frac{ |\bar{y}-y^{*}| } { \bar{y} \cdot y_{\sigma} }\end{split}
\end{equation*}\index{average (cp.nonconformity.AbsErrorKNN attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorKNN.average}\pysigline{\bfcode{average}}
\emph{bool} -- Normalize by average.

\end{fulllineitems}

\index{variance (cp.nonconformity.AbsErrorKNN attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorKNN.variance}\pysigline{\bfcode{variance}}
\emph{bool} -- Normalize by variance.

\end{fulllineitems}

\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cr} \PYG{o}{=} \PYG{n}{CrossRegressor}\PYG{p}{(}\PYG{n}{AbsErrorKNN}\PYG{p}{(}\PYG{n}{Euclidean}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{average}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{cr}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.nonconformity.AbsErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorKNN.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{distance}, \emph{k}, \emph{average=False}, \emph{variance=False}}{}
Initialize the distance measure, number of nearest neighbours to consider and
whether to normalize by average and by variance.

\end{fulllineitems}

\index{stats() (cp.nonconformity.AbsErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorKNN.stats}\pysiglinewithargsret{\bfcode{stats}}{\emph{instance}}{}
Computes mean and standard deviation of values within the k nearest neighbours.

\end{fulllineitems}

\index{norm() (cp.nonconformity.AbsErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorKNN.norm}\pysiglinewithargsret{\bfcode{norm}}{\emph{avg}, \emph{std}}{}
Compute the normalization factor according to the chosen properties.

\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.AbsErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorKNN.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
\end{fulllineitems}

\index{predict() (cp.nonconformity.AbsErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AbsErrorKNN.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
\end{fulllineitems}


\end{fulllineitems}

\index{AvgErrorKNN (class in cp.nonconformity)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AvgErrorKNN}\pysiglinewithargsret{\strong{class }\code{cp.nonconformity.}\bfcode{AvgErrorKNN}}{\emph{distance}, \emph{k=1}}{}
Bases: {\hyperref[cp.nonconformity:cp.nonconformity.RegrNearestNeighboursNC]{\crossref{\code{cp.nonconformity.RegrNearestNeighboursNC}}}}

Average error of k nearest neighbours computes the average absolute error of the actual value (\(y^{*}\))
compared to the k nearest neighbours (\(y_i\)).
\begin{equation*}
\begin{split}\mathit{nc} = 1/k \sum_{N_k(x^{*})} |y^{*} - y_i|\end{split}
\end{equation*}
\begin{notice}{note}{Note:}
There might be no suitable \titleref{y} values for the required significance level at the time of prediction.
In such cases, the predicted range is {[}nan, nan{]}.
\end{notice}
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{cr} \PYG{o}{=} \PYG{n}{CrossRegressor}\PYG{p}{(}\PYG{n}{AvgErrorKNN}\PYG{p}{(}\PYG{n}{Euclidean}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{cr}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{avg\_abs() (cp.nonconformity.AvgErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AvgErrorKNN.avg_abs}\pysiglinewithargsret{\bfcode{avg\_abs}}{\emph{y}, \emph{ys}}{}
\end{fulllineitems}

\index{avg\_abs\_inv() (cp.nonconformity.AvgErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AvgErrorKNN.avg_abs_inv}\pysiglinewithargsret{\bfcode{avg\_abs\_inv}}{\emph{nc}, \emph{ys}}{}
\end{fulllineitems}

\index{nonconformity() (cp.nonconformity.AvgErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AvgErrorKNN.nonconformity}\pysiglinewithargsret{\bfcode{nonconformity}}{\emph{instance}}{}
\end{fulllineitems}

\index{predict() (cp.nonconformity.AvgErrorKNN method)}

\begin{fulllineitems}
\phantomsection\label{cp.nonconformity:cp.nonconformity.AvgErrorKNN.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{inst}, \emph{nc}}{}
\end{fulllineitems}


\end{fulllineitems}



\section{cp.regression}
\label{cp.regression:cp-regression}\label{cp.regression::doc}\label{cp.regression:module-cp.regression}\index{cp.regression (module)}
Regression module contains methods for conformal regression.

Conformal regressors predict a range of values (not always a single value) under a given
significance level (error rate). Every regressors works in combination with a nonconformity measure
and on average predicts the correct value with the given error rate. Lower error rates result in
narrower ranges of predicted values.

Structure:
\begin{itemize}
\item {} \begin{description}
\item[{ConformalRegressor}] \leavevmode\begin{itemize}
\item {} 
Inductive ({\hyperref[cp.regression:cp.regression.InductiveRegressor]{\crossref{\code{InductiveRegressor}}}})

\item {} 
Cross ({\hyperref[cp.regression:cp.regression.CrossRegressor]{\crossref{\code{CrossRegressor}}}})

\end{itemize}

\end{description}

\end{itemize}
\index{PredictionRegr (class in cp.regression)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.PredictionRegr}\pysiglinewithargsret{\strong{class }\code{cp.regression.}\bfcode{PredictionRegr}}{\emph{lo}, \emph{hi}}{}
Bases: \code{object}

Conformal regression prediction object,
which is produced by the {\hyperref[cp.regression:cp.regression.ConformalRegressor.predict]{\crossref{\code{ConformalRegressor.predict()}}}} method.
\index{lo (cp.regression.PredictionRegr attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.PredictionRegr.lo}\pysigline{\bfcode{lo}}
\emph{float} -- Lowest value of the predicted range.

\end{fulllineitems}

\index{hi (cp.regression.PredictionRegr attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.PredictionRegr.hi}\pysigline{\bfcode{hi}}
\emph{float} -- Highest value of the predicted range.

\end{fulllineitems}

\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ccr} \PYG{o}{=} \PYG{n}{CrossRegressor}\PYG{p}{(}\PYG{n}{AbsError}\PYG{p}{(}\PYG{n}{LinearRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{prediction} \PYG{o}{=} \PYG{n}{ccr}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{prediction}\PYG{o}{.}\PYG{n}{width}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.regression.PredictionRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.PredictionRegr.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{lo}, \emph{hi}}{}
Initialize the prediction.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{lo}} (\emph{\texttt{float}}) -- Lowest value of the predicted range.

\item {} 
\textbf{\texttt{hi}} (\emph{\texttt{float}}) -- Highest value of the predicted range.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{range() (cp.regression.PredictionRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.PredictionRegr.range}\pysiglinewithargsret{\bfcode{range}}{}{}
Predicted range: {\hyperref[cp.regression:cp.regression.PredictionRegr.lo]{\crossref{\code{lo}}}}, {\hyperref[cp.regression:cp.regression.PredictionRegr.hi]{\crossref{\code{hi}}}}.

\end{fulllineitems}

\index{verdict() (cp.regression.PredictionRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.PredictionRegr.verdict}\pysiglinewithargsret{\bfcode{verdict}}{\emph{ref}}{}
Conformal regression prediction is correct when the actual value appears
in the predicted range.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{ref}} -- Reference/actual value

\item[{Returns}] \leavevmode
True if the prediction is correct.

\end{description}\end{quote}

\end{fulllineitems}

\index{width() (cp.regression.PredictionRegr method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.PredictionRegr.width}\pysiglinewithargsret{\bfcode{width}}{}{}
Width of the predicted range: {\hyperref[cp.regression:cp.regression.PredictionRegr.hi]{\crossref{\code{hi}}}} - {\hyperref[cp.regression:cp.regression.PredictionRegr.lo]{\crossref{\code{lo}}}}.

\end{fulllineitems}


\end{fulllineitems}

\index{ConformalRegressor (class in cp.regression)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.ConformalRegressor}\pysiglinewithargsret{\strong{class }\code{cp.regression.}\bfcode{ConformalRegressor}}{\emph{nc\_measure}}{}
Bases: {\hyperref[cp.base:cp.base.ConformalPredictor]{\crossref{\code{cp.base.ConformalPredictor}}}}

Base class for conformal regression.
\index{\_\_init\_\_() (cp.regression.ConformalRegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.ConformalRegressor.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}}{}
Verify that the nonconformity measure can be used for regression.

\end{fulllineitems}

\index{predict() (cp.regression.ConformalRegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.ConformalRegressor.predict}\pysiglinewithargsret{\bfcode{predict}}{\emph{example}, \emph{eps}}{}
Compute a regression prediction object for a given example and significance level.

Function determines what is the \code{eps}-th lowest nonconformity score and computes
the range of values that would result in a lower or equal nonconformity. This inverse
of the nonconformity score is computed by the nonconformity measure's
{\hyperref[cp.nonconformity:cp.nonconformity.RegrNC.predict]{\crossref{\code{cp.nonconformity.RegrNC.predict()}}}} function.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{example}} (\emph{\texttt{ndarray}}) -- Attributes array.

\item {} 
\textbf{\texttt{eps}} (\emph{\texttt{float}}) -- Default significance level (error rate).

\end{itemize}

\item[{Returns}] \leavevmode
Regression prediction object.

\item[{Return type}] \leavevmode
{\hyperref[cp.regression:cp.regression.PredictionRegr]{\crossref{PredictionRegr}}}

\end{description}\end{quote}

\end{fulllineitems}

\index{\_\_call\_\_() (cp.regression.ConformalRegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.ConformalRegressor.__call__}\pysiglinewithargsret{\bfcode{\_\_call\_\_}}{\emph{example}, \emph{eps}}{}
Compute predicted range for a given example and significance level.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{example}} (\emph{\texttt{ndarray}}) -- Attributes array.

\item {} 
\textbf{\texttt{eps}} (\emph{\texttt{float}}) -- Significance level (error rate).

\end{itemize}

\item[{Returns}] \leavevmode
Predicted range as a pair (\titleref{PredictionRegr.lo}, \titleref{PredictionRegr.hi})

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{TransductiveRegressor (class in cp.regression)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.TransductiveRegressor}\pysiglinewithargsret{\strong{class }\code{cp.regression.}\bfcode{TransductiveRegressor}}{\emph{nc\_measure}}{}
Bases: {\hyperref[cp.regression:cp.regression.ConformalRegressor]{\crossref{\code{cp.regression.ConformalRegressor}}}}

Transductive regression. TODO

\end{fulllineitems}

\index{InductiveRegressor (class in cp.regression)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.InductiveRegressor}\pysiglinewithargsret{\strong{class }\code{cp.regression.}\bfcode{InductiveRegressor}}{\emph{nc\_measure}, \emph{train=None}, \emph{calibrate=None}}{}
Bases: {\hyperref[cp.regression:cp.regression.ConformalRegressor]{\crossref{\code{cp.regression.ConformalRegressor}}}}

Inductive regression.
\index{alpha (cp.regression.InductiveRegressor attribute)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.InductiveRegressor.alpha}\pysigline{\bfcode{alpha}}
Nonconformity scores of the calibration instances. Computed by the {\hyperref[cp.regression:cp.regression.InductiveRegressor.fit]{\crossref{\code{fit()}}}} method.
Must be \emph{sorted} in increasing order.

\end{fulllineitems}

\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{calibrate} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{RandomSampler}\PYG{p}{(}\PYG{n}{train}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{icr} \PYG{o}{=} \PYG{n}{InductiveRegressor}\PYG{p}{(}\PYG{n}{AbsError}\PYG{p}{(}\PYG{n}{LinearRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{,} \PYG{n}{calibrate}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{icr}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.regression.InductiveRegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.InductiveRegressor.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}, \emph{train=None}, \emph{calibrate=None}}{}
Initialize inductive regressor with a nonconformity measure, training set and calibration set.
If present, fit the conformal regressor to the training set and compute the nonconformity scores of
calibration set.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{nc\_measure}} ({\hyperref[cp.nonconformity:cp.nonconformity.RegrNC]{\crossref{\emph{\texttt{RegrNC}}}}}) -- Regression nonconformity measure.

\item {} 
\textbf{\texttt{train}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a training set.

\item {} 
\textbf{\texttt{calibrate}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a calibration set.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{fit() (cp.regression.InductiveRegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.InductiveRegressor.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{train}, \emph{calibrate}}{}
Fit the conformal regressor to the training set, compute and store sorted nonconformity scores ({\hyperref[cp.regression:cp.regression.InductiveRegressor.alpha]{\crossref{\code{alpha}}}})
on the calibration set and store the domain.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{train}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a training set.

\item {} 
\textbf{\texttt{calibrate}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a calibration set.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{CrossRegressor (class in cp.regression)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.CrossRegressor}\pysiglinewithargsret{\strong{class }\code{cp.regression.}\bfcode{CrossRegressor}}{\emph{nc\_measure}, \emph{k}, \emph{train=None}}{}
Bases: {\hyperref[cp.regression:cp.regression.InductiveRegressor]{\crossref{\code{cp.regression.InductiveRegressor}}}}

Cross regression.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ccr} \PYG{o}{=} \PYG{n}{CrossRegressor}\PYG{p}{(}\PYG{n}{AbsError}\PYG{p}{(}\PYG{n}{LinearRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ccr}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.regression.CrossRegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.CrossRegressor.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}, \emph{k}, \emph{train=None}}{}
Initialize cross regressor with a nonconformity measure, number of folds and training set.
If present, fit the conformal regressor to the training set.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\textbf{\texttt{nc\_measure}} ({\hyperref[cp.nonconformity:cp.nonconformity.RegrNC]{\crossref{\emph{\texttt{RegrNC}}}}}) -- Regression nonconformity measure.

\item {} 
\textbf{\texttt{k}} (\emph{\texttt{int}}) -- Number of folds.

\item {} 
\textbf{\texttt{train}} (\emph{\texttt{Optional{[}Table{]}}}) -- Table of examples used as a training set.

\end{itemize}

\end{description}\end{quote}

\end{fulllineitems}

\index{fit() (cp.regression.CrossRegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.CrossRegressor.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{train}}{}
Fit the cross regressor to the training set. Split the training set into k folds for use as
training and calibration set with an inductive regressor. Concatenate the computed nonconformity scores
and store them ({\hyperref[cp.regression:cp.regression.InductiveRegressor.alpha]{\crossref{\code{InductiveRegressor.alpha}}}}).
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\textbf{\texttt{train}} (\emph{\texttt{Table}}) -- Table of examples used as a training set.

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{LOORegressor (class in cp.regression)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.LOORegressor}\pysiglinewithargsret{\strong{class }\code{cp.regression.}\bfcode{LOORegressor}}{\emph{nc\_measure}, \emph{train=None}}{}
Bases: {\hyperref[cp.regression:cp.regression.CrossRegressor]{\crossref{\code{cp.regression.CrossRegressor}}}}

Leave-one-out regressor is a cross conformal regressor with the number of folds equal
to the size of the training set.
\paragraph{Examples}

\begin{Verbatim}[commandchars=\\\{\}]
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{LOOSampler}\PYG{p}{(}\PYG{n}{Table}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{housing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n}{ccr} \PYG{o}{=} \PYG{n}{LOORegressor}\PYG{p}{(}\PYG{n}{AbsError}\PYG{p}{(}\PYG{n}{LinearRegressionLearner}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{train}\PYG{p}{)}
\PYG{g+gp}{\PYGZgt{}\PYGZgt{}\PYGZgt{} }\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ccr}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{x}\PYG{p}{,} \PYG{l+m+mf}{0.1}\PYG{p}{)}\PYG{p}{)}
\end{Verbatim}
\index{\_\_init\_\_() (cp.regression.LOORegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.LOORegressor.__init__}\pysiglinewithargsret{\bfcode{\_\_init\_\_}}{\emph{nc\_measure}, \emph{train=None}}{}
\end{fulllineitems}

\index{fit() (cp.regression.LOORegressor method)}

\begin{fulllineitems}
\phantomsection\label{cp.regression:cp.regression.LOORegressor.fit}\pysiglinewithargsret{\bfcode{fit}}{\emph{train}}{}
\end{fulllineitems}


\end{fulllineitems}



\section{cp.utils}
\label{cp.utils:module-cp.utils}\label{cp.utils:cp-utils}\label{cp.utils::doc}\index{cp.utils (module)}
Utils module contains various utility functions that are used in different parts
of the conformal prediction library.
\index{get\_instance() (in module cp.utils)}

\begin{fulllineitems}
\phantomsection\label{cp.utils:cp.utils.get_instance}\pysiglinewithargsret{\code{cp.utils.}\bfcode{get\_instance}}{\emph{data}, \emph{i}}{}
Extract a single instance from data as a test instance and return the remainder as a training set.

\end{fulllineitems}

\index{split\_data() (in module cp.utils)}

\begin{fulllineitems}
\phantomsection\label{cp.utils:cp.utils.split_data}\pysiglinewithargsret{\code{cp.utils.}\bfcode{split\_data}}{\emph{data}, \emph{a}, \emph{b}}{}
``Split data in approximate ratio a:b.

\end{fulllineitems}

\index{shuffle\_data() (in module cp.utils)}

\begin{fulllineitems}
\phantomsection\label{cp.utils:cp.utils.shuffle_data}\pysiglinewithargsret{\code{cp.utils.}\bfcode{shuffle\_data}}{\emph{data}}{}
Randomly shuffle data instances.

\end{fulllineitems}



\chapter{Indices and tables}
\label{index:indices-and-tables}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}

\begin{thebibliography}{Shafer05}
\bibitem[Vovk08]{Vovk08}{\phantomsection\label{tutorial:vovk08} 
Glenn Shafer, Vladimir Vovk. Tutorial on Conformal Predictions. \emph{Journal of Machine Learning Research} 9 (2008) 371-421
}
\bibitem[Shafer05]{Shafer05}{\phantomsection\label{tutorial:shafer05} 
Vladimir Vovk, Alex Gammerman, and Glenn Shafer. \emph{Algorithmic Learning in a Random World}. Springer, New York, 2005.
}
\end{thebibliography}


\renewcommand{\indexname}{Python Module Index}
\begin{theindex}
\def\bigletter#1{{\Large\sffamily#1}\nopagebreak\vspace{1mm}}
\bigletter{c}
\item {\texttt{cp.base}}, \pageref{cp.base:module-cp.base}
\item {\texttt{cp.classification}}, \pageref{cp.classification:module-cp.classification}
\item {\texttt{cp.evaluation}}, \pageref{cp.evaluation:module-cp.evaluation}
\item {\texttt{cp.nonconformity}}, \pageref{cp.nonconformity:module-cp.nonconformity}
\item {\texttt{cp.regression}}, \pageref{cp.regression:module-cp.regression}
\item {\texttt{cp.utils}}, \pageref{cp.utils:module-cp.utils}
\end{theindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}
