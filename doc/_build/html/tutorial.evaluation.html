<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Evaluation &mdash; Orange - Conformal Prediction 1.0 documentation</title>
    
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="top" title="Orange - Conformal Prediction 1.0 documentation" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head>
  <body role="document">
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="evaluation">
<h1>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">Â¶</a></h1>
<p>The evaluation module provides many useful classes and functions for evaluating the performance and validity of conformal predictions.
The main two classes, which represent the results of a conformal classifier and regressor, are <a class="reference internal" href="cp.evaluation.html#cp.evaluation.ResultsClass" title="cp.evaluation.ResultsClass"><code class="xref py py-class docutils literal"><span class="pre">cp.evaluation.ResultsClass</span></code></a> and <a class="reference internal" href="cp.evaluation.html#cp.evaluation.ResultsRegr" title="cp.evaluation.ResultsRegr"><code class="xref py py-class docutils literal"><span class="pre">cp.evaluation.ResultsRegr</span></code></a>.</p>
<p>For ease of use, the evaluation results can be obtained using utility functions that evaluate the selected conformal predictor on data defined by the provided sampler (<a class="reference internal" href="cp.evaluation.html#cp.evaluation.run" title="cp.evaluation.run"><code class="xref py py-func docutils literal"><span class="pre">cp.evaluation.run()</span></code></a>) or explicitly provided by the user (<a class="reference internal" href="cp.evaluation.html#cp.evaluation.run_train_test" title="cp.evaluation.run_train_test"><code class="xref py py-func docutils literal"><span class="pre">cp.evaluation.run_train_test()</span></code></a>).</p>
<p>As an example, let&#8217;s take a look at how to quickly evaluate a conformal classifier on a test data set and compute some of the performance metrics:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">Orange</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">cp</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Table</span><span class="p">(</span><span class="s1">&#39;iris&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[::</span><span class="mi">2</span><span class="p">],</span> <span class="n">iris</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lr</span> <span class="o">=</span> <span class="n">Orange</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">LogisticRegressionLearner</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ip</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">nonconformity</span><span class="o">.</span><span class="n">InverseProbability</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ccp</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">CrossClassifier</span><span class="p">(</span><span class="n">ip</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">res</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">evaluation</span><span class="o">.</span><span class="n">run_train_test</span><span class="p">(</span><span class="n">ccp</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
<p>The results are an instance of <a class="reference internal" href="cp.evaluation.html#cp.evaluation.ResultsClass" title="cp.evaluation.ResultsClass"><code class="xref py py-class docutils literal"><span class="pre">cp.evaluation.ResultsClass</span></code></a> mentioned above, and can be used to compute the accuracy of predictions (fraction of predictions including the actual class). For a <em>valid</em> predictor it needs to hold that the error (1 - accuracy) is lower or equal to the specified significance level.
In addition to <em>validity</em>, we are often interested in the <em>efficiency</em> of a predictor. For classification, this is often measured with the fraction of cases with a single predicted class (<a class="reference internal" href="cp.evaluation.html#cp.evaluation.ResultsClass.singleton_criterion" title="cp.evaluation.ResultsClass.singleton_criterion"><code class="xref py py-func docutils literal"><span class="pre">cp.evaluation.ResultsClass.singleton_criterion()</span></code></a>). For regression, one might measure the widths of predicted intervals and e.g. report the average value (<a class="reference internal" href="cp.evaluation.html#cp.evaluation.ResultsRegr.mean_range" title="cp.evaluation.ResultsRegr.mean_range"><code class="xref py py-func docutils literal"><span class="pre">cp.evaluation.ResultsRegr.mean_range()</span></code></a>).</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Accuracy:&#39;</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">accuracy</span><span class="p">())</span>
<span class="go">Accuracy: 0.946666666667</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Singletons:&#39;</span><span class="p">,</span> <span class="n">res</span><span class="o">.</span><span class="n">singleton_criterion</span><span class="p">())</span>
<span class="go">Singletons: 0.96</span>
</pre></div>
</div>
<p>Another very useful visual validation approach is to plot the dependency of the actual measured error rate at different levels of the specified significance so the user can quickly see that the error is indeed controlled by the parameter.
There is a function in the evaluation module that prepares a calibration plot for the specified predictor and data:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cp</span><span class="o">.</span><span class="n">evaluation</span><span class="o">.</span><span class="n">calibration_plot</span><span class="p">(</span><span class="n">ccp</span><span class="p">,</span> <span class="n">iris</span><span class="p">,</span> <span class="n">fname</span><span class="o">=</span><span class="s1">&#39;calibration.png&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper"><div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/tutorial.evaluation.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2016, Biolab.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.4.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.8</a>
      
      |
      <a href="_sources/tutorial.evaluation.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>